# -*- coding: utf-8 -*-
"""Fetal Health Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17-Gt3kVdS8h2Xoh2yCK2D8t6ZhpjpY3l

# Fetal Health Classification - By Prateek Dutta

Classify fetal health in order to prevent child and maternal mortality.

Reduction of child mortality is reflected in several of the United Nations' Sustainable Development Goals and is a key indicator of human progress. The UN expects that by 2030, countries end preventable deaths of newborns and children under 5 years of age, with all countries aiming to reduce underâ€‘5 mortality to at least as low as 25 per 1,000 live births.

Parallel to notion of child mortality is of course maternal mortality, which accounts for 295 000 deaths during and following pregnancy and childbirth (as of 2017). The vast majority of these deaths (94%) occurred in low-resource settings, and most could have been prevented.

In light of what was mentioned above, Cardiotocograms (CTGs) are a simple and cost accessible option to assess fetal health, allowing healthcare professionals to take action in order to prevent child and maternal mortality. The equipment itself works by sending ultrasound pulses and reading its response, thus shedding light on fetal heart rate (FHR), fetal movements, uterine contractions and more.

It is a Classification Task.

Algorithms & Techniques Used:-
Principle Component Analysis (PCA)

KNN,

Random Forest Classifier,

Support Vector Machine (SVM)

Logistic Regresiion
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive/')
# %cd /gdrive

ls

cd/gdrive/MyDrive/Fetal Health classification/

ls

import pandas as pd
import numpy as np
data=pd.read_csv("fetal_health.csv")
data.head()

data.info()

data.describe()

"""**Minor Observation:** Attributes such as severe_decelerations,prolongues_decelerations are more or less constant with few variation. We may remove those. Have taken care of these in Dimensionality Reduction."""

data.isnull().sum()

X=data.drop(['fetal_health'],axis=1)
y=data['fetal_health']

"""# DATA VISUALIZATIONS AND EDA"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style(style='darkgrid')
sns.countplot(data=data,x='fetal_health')
plt.title("Number of samples of each class")

"""Observation: An highly Imbalanced Dataset. Which is obvious as Normal would be dominant.

Solution: Oversampling of the minority classes. To make better predictions.(Will perform later in the notebook)
"""

corr=X.corr()
plt.figure(figsize=(15,15))
sns.heatmap(corr,annot=True)

"""Observations: > histogram mean,median,mode have high correlations.The distribution may towards normal."""

grouped=data.groupby(by='fetal_health').mean()
grouped

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline
sns.set()
for index,i in enumerate(grouped.columns,start=1):
    plt.figure(figsize=(6,4))
    sns.barplot(data=grouped,x=grouped.index,y=grouped[i])
    plt.show()

"""# Oversampling of Minority Class"""

from imblearn.over_sampling import RandomOverSampler
oversample = RandomOverSampler(sampling_strategy='not majority')
X_over, y_over = oversample.fit_resample(X, y)

y_over

X_over

sns.countplot(data=pd.DataFrame(y_over),x='fetal_health')

"""Now All the classes have same number of samples! Lets do some Dimensionality reduction to get more insights

# Dimensionality Reduction and visualization

First lets scale the data!
"""

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_scaled=scaler.fit_transform(X_over)

X_scaled=pd.DataFrame(X_scaled,columns=X_over.columns)
X_scaled.head()

from sklearn.decomposition import PCA
pca=PCA(n_components=3) #getting 3 components with highest variance
X_pca=pca.fit_transform(X_scaled)
X_pca=pd.DataFrame(X_pca)
X_pca.head()

import plotly.express as px
fig = px.scatter_3d(X_pca, x=0, y=1, z=2,
              color=y_over, size_max=10)
fig.show()

"""Not much information by checking this graph!"""

from sklearn.decomposition import PCA
pca=PCA(n_components=0.95) #0.95 here refers that the total variance explained by the components must be atleast 95%
X_pca_final=pca.fit_transform(X_scaled)
X_pca_final=pd.DataFrame(X_pca_final)
X_pca_final.head()

"""# Model Building"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X_pca_final,y_over,random_state=42,stratify=y_over)

"""## K-Nearest Neighbor"""

from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
knn_scores=[]
for k in range(1,20):
    knn=KNeighborsClassifier(n_neighbors=k)
    scores=cross_val_score(knn,X_train,y_train,cv=5)
    knn_scores.append(scores.mean())

x_ticks = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]
x_labels = x_ticks

plt.plot([k for k in range(1,20)],knn_scores)
plt.xticks(ticks=x_ticks, labels=x_labels)
plt.grid()

knn=KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)
from sklearn.metrics import confusion_matrix
confusion_knn=confusion_matrix(y_test,knn.predict(X_test))
sns.heatmap(confusion_knn,annot=True)

from sklearn.metrics import classification_report
print(classification_report(y_test,knn.predict(X_test)))

"""Although got 98% still not satisfied because neighbor=1 usually mean my model is very much dependent on just its neighbor may not work on new data. Lets try new models

# Support Vector Machine
"""

param_grid={'C':[0.001,0.01,0.1,1,10,100], 'gamma':[0.001,0.01,0.1,1,10,100]}
rcv=RandomizedSearchCV(SVC(),param_grid,cv=5)
rcv.fit(X_train,y_train)
y_pred_svc=rcv.predict(X_test)
confusion_svc=confusion_matrix(y_test,rcv.predict(X_test))
sns.heatmap(confusion_svc,annot=True)
print(classification_report(y_test,y_pred_svc))

rcv.best_params_

"""Support Vector Classifier has done a tremendous job!! I dont think we require to move for new models but just for learning lets explore!!

# Logistic Regression
"""

param_grid={'C':[0.001,0.01,0.1,1,10,100], 'max_iter':[30,50,70,150,200,300,400,500,700,1200,1500]}
log=RandomizedSearchCV(LogisticRegression(),param_grid,cv=5)
log.fit(X_train,y_train)
y_pred_log=log.predict(X_test)
confusion_log=confusion_matrix(y_test,log.predict(X_test))
sns.heatmap(confusion_log,annot=True)
print(classification_report(y_test,y_pred_log))

"""# Random Forest Classifier"""

param_grid = {
'n_estimators': [50, 100, 150, 200],
}
rcv=RandomizedSearchCV(RandomForestClassifier(random_state=42),param_grid,cv=5)
rcv.fit(X_train,y_train)
y_pred_rcv=rcv.predict(X_test)
confusion_rcv=confusion_matrix(y_test,rcv.predict(X_test))
sns.heatmap(confusion_rcv,annot=True)
print(classification_report(y_test,y_pred_rcv))

"""# Saving model"""

model=SVC(gamma=100,C=10)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
confusion_svc=confusion_matrix(y_test,y_pred)
sns.heatmap(confusion_svc,annot=True)
print(classification_report(y_test,y_pred_svc))

import pickle
with open('FetalHealthClassificationFinal','wb') as f:
    pickle.dump(model,f)

"""**This project had few main steps**
1. Oversampling of minority class
2. Dimensionality Reduction

**Rest all Data Exploration,Data Preprocessing and Model Building stays the same!**
"""